{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training from scratch v/s Transfer learning\n",
    "This project was a part of **Machine Learning Specialization with TF2** by [**CLOUDXLAB**](http://cloudxlab.com/).\n",
    "\n",
    "### What are we going to do?\n",
    "\n",
    "We will train a neural network (say model A) on data related to 6 of the classes, and we will train another neural network (say model B) on the remaining 2 classes. Then, we would use the pre-trained weights of model A and tune the last layer so as to classify these 2 classes(this technique is called Transfer Learning), and compare the results of classification obtained using normal training and transfer learning. In this project, we would practically appreciate the use of Transfer Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full[:30000]\n",
    "y_train_full = y_train_full[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[:5000]\n",
    "y_test = y_test[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train =X_train_full[:5000], X_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the data sets\n",
    "Let's split the fashion MNIST training set in two:\n",
    "\n",
    "**X_train_A:** all images of all items except for sandals and shirts (classes 5 and 6). **X_train_B:** a much smaller training set of just the first 200 images of sandals or shirts. The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "**Why are we doing this?**\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using Dense layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the **split_dataset function**, which splits the whole dataset into 2: one which contains sandals and shirts data, the other containing the images of the remaining classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A), (X[y_5_or_6], y_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Fit the Model A\n",
    "Let us define the model for the classification of data set A that we have created previously.\n",
    "\n",
    "Later the trained weights of this model will be used for the classification task of data B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer= keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19875 samples, validate on 4014 samples\n",
      "Epoch 1/5\n",
      "19875/19875 [==============================] - 2s 126us/sample - loss: 0.8034 - accuracy: 0.7492 - val_loss: 0.4698 - val_accuracy: 0.8508\n",
      "Epoch 2/5\n",
      "19875/19875 [==============================] - 2s 86us/sample - loss: 0.4269 - accuracy: 0.8593 - val_loss: 0.4009 - val_accuracy: 0.8642\n",
      "Epoch 3/5\n",
      "19875/19875 [==============================] - 2s 85us/sample - loss: 0.3715 - accuracy: 0.8725 - val_loss: 0.3487 - val_accuracy: 0.8802\n",
      "Epoch 4/5\n",
      "19875/19875 [==============================] - 2s 85us/sample - loss: 0.3419 - accuracy: 0.8812 - val_loss: 0.3296 - val_accuracy: 0.8876\n",
      "Epoch 5/5\n",
      "19875/19875 [==============================] - 2s 90us/sample - loss: 0.3246 - accuracy: 0.8855 - val_loss: 0.3258 - val_accuracy: 0.8849\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=5,\n",
    "            validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Fit the Model B\n",
    "Let us define the model for the classification of data set B that we have created previously.\n",
    "\n",
    "Later, let us also examine the classification of B set by using the trained weights of model A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss= \"binary_crossentropy\",\n",
    "    optimizer= keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5125 samples, validate on 986 samples\n",
      "Epoch 1/5\n",
      "5125/5125 [==============================] - 1s 277us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "5125/5125 [==============================] - 0s 96us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "5125/5125 [==============================] - 1s 115us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "5125/5125 [==============================] - 1s 121us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "5125/5125 [==============================] - 0s 94us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=5,\n",
    "            validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new model based on existing model A\n",
    "Let us first see how many trainable parameters are there for model_B we trained previously.\n",
    "\n",
    "Then we shall create a new model model_B_on_A which has the pre-trained parameters of model_A but customized final dense layer with only 1 neuron.\n",
    "\n",
    "Finally, we shall compare the performance of both the models - model_B and model_B_on_A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before creating model_B_on_A(a model based on pre-trained layers of model_A), we shall clone the model_A and set its trained weights so that when you train model_B_on_A, it will not affect model_A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model model_B_on_A, based on existing layers of model_A\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 51\n",
      "Non-trainable params: 275,750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B_on_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "         optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "         metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5125 samples, validate on 986 samples\n",
      "Epoch 1/5\n",
      "5125/5125 [==============================] - 1s 177us/sample - loss: 0.3706 - accuracy: 0.8353 - val_loss: 0.2759 - val_accuracy: 0.9128\n",
      "Epoch 2/5\n",
      "5125/5125 [==============================] - 0s 73us/sample - loss: 0.2165 - accuracy: 0.9419 - val_loss: 0.1872 - val_accuracy: 0.9554\n",
      "Epoch 3/5\n",
      "5125/5125 [==============================] - 0s 72us/sample - loss: 0.1553 - accuracy: 0.9707 - val_loss: 0.1453 - val_accuracy: 0.9716\n",
      "Epoch 4/5\n",
      "5125/5125 [==============================] - 0s 74us/sample - loss: 0.1234 - accuracy: 0.9811 - val_loss: 0.1213 - val_accuracy: 0.9797\n",
      "Epoch 5/5\n",
      "5125/5125 [==============================] - 0s 73us/sample - loss: 0.1039 - accuracy: 0.9852 - val_loss: 0.1059 - val_accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=5,\n",
    "                   validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the models\n",
    "Now that we have the two models model_B and model_B_on_A for classifying the B dataset, let us evaluate the performance of the model based on their accuracies on the test data of B data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967/967 [==============================] - 0s 49us/sample - loss: 7.6483 - accuracy: 0.4984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.64827382502906, 0.49844882]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967/967 [==============================] - 0s 55us/sample - loss: 0.0992 - accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09924376358383075, 0.98862463]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the accuracies of both models are almost the same.\n",
    "\n",
    "We also see that the performance of model_B_on_A - with as less as 51 trainable parameter - stands to be as great as that of model_Bwith as many as 275,801.\n",
    "\n",
    "So, with very little training, model_B_on_A is performing really well. This saves time and resources even in real-time scenarios. This is the beauty of using pre-trained layers. This method is also known as transfer learning - transferring the knowledge obtained from solving one problem to solving another similar problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
