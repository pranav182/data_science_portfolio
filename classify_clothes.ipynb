{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify clothes using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset\n",
    "Fashion-MNIST is a dataset of Zalando's article images, consisting of a **training set of 60,000 examples** and a **test set of 10,000 examples.** Each example is a **28x28 grayscale image**, associated with a label from **10 classes.** Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. Zalando seeks to replace the original MNIST dataset\n",
    "\n",
    "- Training set - 60,000 examples\n",
    "- Test set - 10,000 examples\n",
    "- Each example is a 28x28 grayscale image\n",
    "- 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content of Dataset\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. - Each row is a separate image - Column 1 is the class label. - Remaining columns are pixel numbers (784 total) - Each value is the darkness of the pixel (1 to 255)\n",
    "\n",
    "### Labels\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "0 - T-shirt/top\n",
    "\n",
    "1 - Trouser\n",
    "\n",
    "2 - Pullover\n",
    "\n",
    "3 - Dress\n",
    "\n",
    "4 - Coat\n",
    "\n",
    "5 - Sandal\n",
    "\n",
    "6 - Shirt\n",
    "\n",
    "7 - Sneaker\n",
    "\n",
    "8 - Bag\n",
    "\n",
    "9 - Ankle boot\n",
    "\n",
    "### Objective\n",
    "Train a CNN model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import tensorflow as tf\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def load_data():\n",
    "    \n",
    "    filePath_train_set = '/cxldata/datasets/project/fashion-mnist/train-images-idx3-ubyte.gz'\n",
    "\n",
    "    filePath_train_label = '/cxldata/datasets/project/fashion-mnist/train-labels-idx1-ubyte.gz'\n",
    "\n",
    "    filePath_test_set = '/cxldata/datasets/project/fashion-mnist/t10k-images-idx3-ubyte.gz'\n",
    "\n",
    "    filePath_test_label = '/cxldata/datasets/project/fashion-mnist/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "    with gzip.open(filePath_train_label, 'rb') as trainLbpath:\n",
    "         trainLabel = np.frombuffer(trainLbpath.read(), dtype=np.uint8,\n",
    "                                   offset=8)\n",
    "    with gzip.open(filePath_train_set, 'rb') as trainSetpath:\n",
    "         trainSet = np.frombuffer(trainSetpath.read(), dtype=np.uint8,\n",
    "                                   offset=16).reshape(len(trainLabel), 28, 28)\n",
    "\n",
    "    with gzip.open(filePath_test_label, 'rb') as testLbpath:\n",
    "         testLabel = np.frombuffer(testLbpath.read(), dtype=np.uint8,\n",
    "                                   offset=8)\n",
    "\n",
    "    with gzip.open(filePath_test_set, 'rb') as testSetpath:\n",
    "         testSet = np.frombuffer(testSetpath.read(), dtype=np.uint8,\n",
    "                                   offset=16).reshape(len(testLabel), 28, 28)\n",
    "\n",
    "    trainX = trainSet.copy()\n",
    "    testX = testSet.copy()\n",
    "    trainY = trainLabel.copy()\n",
    "    testY = testLabel.copy()\n",
    "    \n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255.\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape =  (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Add dimension at the end as images are greyscale.\n",
    "input_shape = (x_train.shape[1:] + (1,)) # (28, 28, 1) -> 2D CNNs accept 3D input tensors.\n",
    "print(\"Input shape = \", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our labels to one-hot encoded form\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1120 11:37:06.469989 139785740138304 deprecation.py:506] From /usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model using Functional API\n",
    "inp = Input(shape=input_shape)\n",
    "_ = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inp)\n",
    "_ = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(_)\n",
    "_ = MaxPool2D(pool_size=(2, 2))(_)\n",
    "_ = Dropout(0.25)(_)\n",
    "_ = Flatten()(_)\n",
    "_ = Dense(units=128, activation='relu')(_)\n",
    "_ = Dropout(0.2)(_)\n",
    "_ = Dense(units=num_classes, activation='softmax')(_)\n",
    "model = Model(inputs=inp, outputs=_)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/12\n",
      "42000/42000 [==============================] - 22s 535us/sample - loss: 0.5002 - acc: 0.8219 - val_loss: 0.3471 - val_acc: 0.8738\n",
      "Epoch 2/12\n",
      "42000/42000 [==============================] - 28s 677us/sample - loss: 0.3122 - acc: 0.8872 - val_loss: 0.2748 - val_acc: 0.9025\n",
      "Epoch 3/12\n",
      "42000/42000 [==============================] - 22s 520us/sample - loss: 0.2580 - acc: 0.9038 - val_loss: 0.2628 - val_acc: 0.9039\n",
      "Epoch 4/12\n",
      "42000/42000 [==============================] - 23s 542us/sample - loss: 0.2246 - acc: 0.9178 - val_loss: 0.2388 - val_acc: 0.9123\n",
      "Epoch 5/12\n",
      "42000/42000 [==============================] - 23s 546us/sample - loss: 0.1977 - acc: 0.9273 - val_loss: 0.2330 - val_acc: 0.9150\n",
      "Epoch 6/12\n",
      "42000/42000 [==============================] - 23s 539us/sample - loss: 0.1729 - acc: 0.9354 - val_loss: 0.2300 - val_acc: 0.9214\n",
      "Epoch 7/12\n",
      "42000/42000 [==============================] - 23s 544us/sample - loss: 0.1529 - acc: 0.9426 - val_loss: 0.2317 - val_acc: 0.9240\n",
      "Epoch 8/12\n",
      "42000/42000 [==============================] - 23s 536us/sample - loss: 0.1321 - acc: 0.9498 - val_loss: 0.2301 - val_acc: 0.9225\n",
      "Epoch 9/12\n",
      "42000/42000 [==============================] - 22s 526us/sample - loss: 0.1178 - acc: 0.9567 - val_loss: 0.2309 - val_acc: 0.9258\n",
      "Epoch 10/12\n",
      "42000/42000 [==============================] - 22s 524us/sample - loss: 0.1067 - acc: 0.9600 - val_loss: 0.2334 - val_acc: 0.9268\n",
      "Epoch 11/12\n",
      "42000/42000 [==============================] - 22s 531us/sample - loss: 0.0903 - acc: 0.9659 - val_loss: 0.2532 - val_acc: 0.9237\n",
      "Epoch 12/12\n",
      "42000/42000 [==============================] - 22s 516us/sample - loss: 0.0853 - acc: 0.9691 - val_loss: 0.2633 - val_acc: 0.9243\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(), metrics=['accuracy']) # categorical_crossentropy loss function for multi-class classification\n",
    "history = model.fit(np.expand_dims(x_train, -1), y_train, batch_size=128, epochs=12, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  0.28951597318053246\n",
      "Accuracy =  0.9165\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "loss, accuracy = model.evaluate(np.expand_dims(x_test, -1), y_test, verbose=0)\n",
    "print(\"Loss = \", loss)\n",
    "print(\"Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
